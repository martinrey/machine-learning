\subsection{Modelado}
El juego en si consistirá en una lista de listas donde cada lista representará una columna del tablero. En cada turno un jugador elejirá una columna numerada del 0 al n, siendo n el numero de columnas totales y el juego se encargará. Ambos jugadores contarán en el momento de la elección con el estado actual del tablero.

Luego de cada jugada, el juego chequará si el jugador ganó o si ya no hay mas movimientos posibles, terminando el juego e informando que jugador gano o si fue empate.

El pseudocodigo del juego será, entonces el siguiente:

\begin{algorithm}[h!]
\begin{algorithmic}[1]\parskip=1mm
 \caption{jugar()}
 \STATE{While True}
 \STATE{\quad columna = player.move(tablero)}
 \STATE{\quad jugar\_ficha(columna, tablero)}
 \STATE{\quad if jugador\_gano(tablero)}
 \STATE{\quad\quad return player}
 \STATE{\quad if tablero\_lleno(tablero)}
 \STATE{\quad\quad return empate}
 \STATE{\quad player = otro\_jugador(player)}
\end{algorithmic}
\end{algorithm}

Como ya adelantamos en la introducción, es necesario definir en que etapa del algoritmo se le asignará la recompensa al algoritmo de q-learning. Una opción bastante sencilla e intuitiva es la de asignar recompensas en el momento en que algun jugador gana la partida. Por ejemplo al ganar la partida uno de los jugadores, se le asigna una recompensa de $1$ a el y una recompensa de $-1$ al contrincante. Siguiendo con el lineamiento anterior, tambien sería posible asignarles recompensas en el momento en que se empata, por ejemplo, asignandoles a ambos jugadores una recompensa de $0.5$

En caso de que no se haya llegado a un estado final, una posible propuesta es asignarle al otro jugador una recompenza por ejemplo de 0.

El pseudocodigo entonces, pasaría a verse de la siguiente manera:

\begin{algorithm}[h!]
\begin{algorithmic}[1]\parskip=1mm
 \caption{jugar()}
 \STATE{While True}
 \STATE{\quad columna = player.move(tablero)}
 \STATE{\quad jugar\_ficha(columna, tablero)}
 \STATE{\quad if jugador\_gano(tablero)}
 \STATE{\quad\quad player.recompensa(1)}
 \STATE{\quad\quad otro\_jugador(player).recompensa(-1)}
 \STATE{\quad\quad return player}
 \STATE{\quad if tablero\_lleno(tablero)}
 \STATE{\quad\quad player.recompensa(0.5)}
 \STATE{\quad\quad otro\_jugador(player).recompensa(0.5)}
 \STATE{\quad\quad return empate}
 \STATE{\quad otro\_jugador(player).recompensa(0)}
 \STATE{\quad player = otro\_jugador(player)}
\end{algorithmic}
\end{algorithm}

En la siguiente sección experimentaremos con distintos algoritmos para elejir el movimiento tomado por el jugador y ver como esto impacta en el parendisaje.

\subsection{Experimentación Con Distintas Estrategias de Movimiento}

En esta sección plantearemos distintas estrategias que utilizará nuestro jugador al momento de elegir si explorar nuevas posibilidades o utilizar el mejor camino conocido.

En particular plantearemos estas tres estrategias:

%capaz no usar itemize?

\begin{itemize}
\item Estrategia Greedy: toma un camino random con probabilidad $\epsilon\%$ y en caso contrario utilice el mejor brazo conocido.
\item Estrategia $\epsilon$-first: toma un camino random en las primeras $\epsilon$ iteraciones y luego toma el mejor camino conocido.
\item Estrategia Softmax: basada en una función probabilistica que desarrollaremos a continuación.
\end{itemize}

La estrategia Softmax se basa en darle probabilidades distintas a cada acción posible dependiendo de la recomensa esperada de cada una de ellas.

En particular la probabilidad de cada una de las acciones vendrá dada por:

%imagen1.svg

 La variable $\tau$ se denomina parametro de temperatura, para temperaturas cercanas a infinito todas las acciones tienen aproximadamente la misma probabilidad de ser elegidas, mientras que para temperaturas bajas (cercanas a cero) la probabilidad de la accion de mayor recompensa tenderá a $1$.

En particular nuestra estrategia consistirá en comenzar con una temperatura alta para favorecer la exploración e ir gradualemente disminuyendola para favorecer mas a aquellas con mayor recopensa. Faltará definir de manera experimental cual será el valor inicial de $\tau$ y de que manera reduciremos su valor (algunas posiblidades son de manera linea, logaritmica, etc).

