Como ya adelantamos en la seccion anterior el objetivo de este tp será clasificar mails en spam y no spam, para ellos, comenzaremos el trabajo seleccionando atributos adecuados que concideramos dividen bien el problema.

\subsection{Selección De atributos}

Para la selección de atributos observamos parte del json entregado en busca de palabras claves que pudieran distinguir entre spam y no spam (desde ahora, ham).

De este analisis se encontró que palabras tales como $viagra$ o $nigeria$ son muy frecuentes en los mensajes de spam, asi tambien aquellas que hagan referencia a negocios o a dinero, por lo que incluimos atributos que cuenten las veces que son mencionados estas palabras en el cuerpo y el encabezado del mensaje. Así tambien observamos que los mails de spam tienden a tener enlaces hacia sitios web o contenido html en el cuerpo del mensaje por lo que tambien contavilizamos la cantidad de ocurrencias de palabras tales como html o http y simbolos especiales como la barra invertida, o el hashtag.

De esta menera reunimos al rededor de $100$ atributos que utilizaremos a continuación para la experimentación.

\subsection{Experimentación}

Los clasificadores que utilizaremos para experimentar en este trabajo serán: 

\begin{itemize}
\item	Arboles de Deciciones
\item	Naive bayes Multinomial
\item	Vecinos mas Cercanos
\item	SVC
\item	Random Forest
\end{itemize}

Para cada uno de ellos utilizaremos grid search para intentar encontrar los hiperparametros que logren la mejor clasificación.

En cada una de las experimentaciónes utilizamos grid search de sklearn con $10$ kfolds.

\subsubsection{Arboles de Deciciones}

Como vimos en clase, este clasificador intentará aplicar una serie de reglas sucesivas para determinar la clasificación. Por ejemplo, un posible arbol de decición para clasificar spam podría ser, si el mensaje contiene la palabra $nigeria$ mas de $4$ veces, es spam, si no ver cuantas veces se menciona $html$ en el mensaje, si el resultado esta entre $4$ y $7$ es spam, si no no... y así sucecivamente.

En particular, los hiperparametros que querremos encontrar para este algoritmo en particular son: la maxima profunididad del arbol, y la cantidad minima de muestras necesarias para dividir un nodo.

Experimentamos variando los hiperparametros entre los siguientes valores:

\begin{itemize}
\item $max_depth: 1,3,5,10,15,50,100$ 
\item $min_samples_split: 1,3,5,10,15$
\end{itemize}

El mejor resultado obtenido fue de $0.965975308642$ con un max_depth de $50$ y un min sammple split de $1$.

\terminar
% concluciones
%Best Score: 0.965975308642 Best Params: {'max_depth': 50, 'min_samples_split': 1}


\subsubsection{Naive bayes Multinomial}

El objetivo del Naive bayes será encontrar la clase mas probable de cada instancia en base al calculo de la probabilidad bayesiana. 

Para este clasificador querremos encontrar el mejor suabizado laplaciano ($alfa$), para ello experimentamos con los siguientes valores:
\begin{itemize}
\item $alpha": 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.8,0.9,1$
\end{itemize}

El resultado del grid search arroja que el mejor score resulto ser de $0.587740740741$ con un alpha de $0.1$

%Best Score: 0.587740740741 Best Params: {'alpha': 0.1}

\subsubsection{Vecinos mas Cercanos}


En este caso el mejor resultado obtenido fue de $0.908358024691$ con una cantidad de vecinos igual a 1 y pesos uniformes.

%Best Score: 0.908358024691 Best Params: {'n_neighbors': 1, 'weights': 'uniform'}

\subsubsection{SVC}

%Sin resultados yet

\subsubsection{Random Forest}


Para este estimador el mejor resultado fue de $0.979098765432$ con una cantidad de arboles igual a $100$, un max_depth variable y una cantidad de features a examinar igual a $10$.

%Best Score: 0.979098765432 Best Params: {'n_estimators': 100, 'max_depth': None, 'max_features': 10}

\subsection{PCA}

Para intentar mejorar la precición de los resultados utilizamos PCA para obtener las componentes principales de nuestro set de atrubutos.


Arboles de Deciciones
Best Score: 0.931592592593 Best Params: {'pca__n_components': 30}
Naive bayes Multinomial
Best Score: 0.596308641975 Best Params: {'pca__n_components': 30}
Vecinos mas Cercanos
Best Score: 0.908 Best Params: {'pca__n_components': 70}
SVC
?
Random Forest
Best Score: 0.967962962963 Best Params: {'pca__n_components': 40}
